{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# process origin dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load origin dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia = datasets.Dataset.from_json(\"/dataset/wikipedia.jsonl\")\n",
    "\n",
    "wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sample dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 2023\n",
    "sample_size = 250_000 + 50_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = wikipedia.shuffle(seed=seed).select(range(250_000, sample_size))\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split sentence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "open_list = [\"[\", \"{\", \"(\"]\n",
    "close_list = [\"]\", \"}\", \")\"]\n",
    "\n",
    "def is_balance(text):\n",
    "    stack = []\n",
    "    for c in text:\n",
    "        if c in open_list:\n",
    "            stack.append(c)\n",
    "        elif c in close_list:\n",
    "            pos = close_list.index(c)\n",
    "            if stack and (open_list[pos] == stack[-1]):\n",
    "                stack.pop()\n",
    "            else:\n",
    "                return False\n",
    "    if len(stack) == 0:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "punctuation_tuple = tuple(string.punctuation)\n",
    "\n",
    "def is_sentence_clean(sent: str, min_length: int=5, max_length: int=128):\n",
    "    sent = sent.strip()\n",
    "    \n",
    "    length = len(sent.split())\n",
    "    if length < min_length or length > max_length:\n",
    "        return False\n",
    "    if sent.startswith(tuple(string.punctuation)):\n",
    "        return False\n",
    "    if \"\\n\" in sent:\n",
    "        return False\n",
    "    if not is_balance(sent):\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.prefer_gpu(4)\n",
    "nlp = spacy.load(\"en_core_web_trf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_list = []\n",
    "length_list = []\n",
    "for text in tqdm(dataset[\"text\"]):\n",
    "    doc = nlp(text)\n",
    "    for sent in doc.sents:\n",
    "        sent = sent.text\n",
    "        sent = sent.strip()\n",
    "        if is_sentence_clean(sent, 8, 128):\n",
    "            sentence_list.append(sent)\n",
    "            length_list.append(len(sent.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sentence_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(length_list) / len(length_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.load_from_disk(\"./tmp/wikipedia_test_medium/\")\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(lambda example: {\"length\": len(example[\"text\"].split())}, num_proc=32)\n",
    "\n",
    "length_list = dataset[\"length\"]\n",
    "\n",
    "sum(length_list) / len(length_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## shuffle & get new dataset list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(sentence_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 4_000_000\n",
    "valid_size = 5_000\n",
    "test_size = 5_000\n",
    "\n",
    "dataset_list = sentence_list[:train_size + valid_size + test_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "medium_test_list = [s for s in sentence_list[train_size + valid_size + test_size:] if 50 > len(s.split()) > 35][:test_size]\n",
    "long_test_list = [s for s in sentence_list[train_size + valid_size + test_size:] if len(s.split()) > 65][:test_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                      \r"
     ]
    }
   ],
   "source": [
    "dataset = datasets.Dataset.from_dict(\n",
    "    {\n",
    "        \"text\": dataset_list,\n",
    "    }\n",
    ")\n",
    "\n",
    "dataset.save_to_disk(\"./tmp/wikipedia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                \r"
     ]
    }
   ],
   "source": [
    "# test medium\n",
    "dataset = datasets.Dataset.from_dict(\n",
    "    {\n",
    "        \"text\": medium_test_list,\n",
    "    }\n",
    ")\n",
    "\n",
    "dataset.save_to_disk(\"./tmp/wikipedia_test_medium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    }
   ],
   "source": [
    "# test long\n",
    "dataset = datasets.Dataset.from_dict(\n",
    "    {\n",
    "        \"text\": long_test_list,\n",
    "    }\n",
    ")\n",
    "\n",
    "dataset.save_to_disk(\"./tmp/wikipedia_test_long\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# process new dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get embedding & save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sup-simcse-bert-base-uncased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!torchrun --nproc_per_node=8 ../embedding/sup-simcse-bert-base-uncased.py \\\n",
    "    --input_dataset \"./tmp/wikipedia\" \\\n",
    "    --output_dataset \"your_output_dir\" \\\n",
    "    --train_size 4000000 \\\n",
    "    --valid_size 5000 \\\n",
    "    --test_size 5000 \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wikipedia_test_medium\n",
    "!torchrun --nproc_per_node=8 ../embedding/sup-simcse-bert-base-uncased.py \\\n",
    "    --input_dataset \"./tmp/wikipedia_test_medium\" \\\n",
    "    --output_dataset \"your_output_dir\" \\\n",
    "    --train_size 0 \\\n",
    "    --valid_size 0 \\\n",
    "    --test_size 5000 \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wikipedia_test_long\n",
    "!torchrun --nproc_per_node=8 ../embedding/sup-simcse-bert-base-uncased.py \\\n",
    "    --input_dataset \"./tmp/wikipedia_test_long\" \\\n",
    "    --output_dataset \"your_output_dir\" \\\n",
    "    --train_size 0 \\\n",
    "    --valid_size 0 \\\n",
    "    --test_size 5000 \\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e5-large-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!torchrun --nproc_per_node=8 ../embedding/e5-large-v2.py \\\n",
    "    --input_dataset \"./tmp/wikipedia\" \\\n",
    "    --output_dataset \"your_output_dir\" \\\n",
    "    --train_size 4000000 \\\n",
    "    --valid_size 5000 \\\n",
    "    --test_size 5000 \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wikipedia_test_medium\n",
    "!torchrun --nproc_per_node=8 ../embedding/e5-large-v2.py \\\n",
    "    --input_dataset \"./tmp/wikipedia_test_medium\" \\\n",
    "    --output_dataset \"your_output_dir\" \\\n",
    "    --train_size 0 \\\n",
    "    --valid_size 0 \\\n",
    "    --test_size 5000 \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wikipedia_test_long\n",
    "!torchrun --nproc_per_node=8 ../embedding/e5-large-v2.py \\\n",
    "    --input_dataset \"./tmp/wikipedia_test_long\" \\\n",
    "    --output_dataset \"your_output_dir\" \\\n",
    "    --train_size 0 \\\n",
    "    --valid_size 0 \\\n",
    "    --test_size 5000 \\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bge-large-en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!torchrun --nproc_per_node=8 ../embedding/bge-large-en.py \\\n",
    "    --input_dataset \"./tmp/wikipedia\" \\\n",
    "    --output_dataset \"your_output_dir\" \\\n",
    "    --train_size 4000000 \\\n",
    "    --valid_size 5000 \\\n",
    "    --test_size 5000 \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wikipedia_test_medium\n",
    "!torchrun --nproc_per_node=8 ../embedding/bge-large-en.py \\\n",
    "    --input_dataset \"./tmp/wikipedia_test_medium\" \\\n",
    "    --output_dataset \"your_output_dir\" \\\n",
    "    --train_size 0 \\\n",
    "    --valid_size 0 \\\n",
    "    --test_size 5000 \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wikipedia_test_long\n",
    "!torchrun --nproc_per_node=8 ../embedding/bge-large-en.py \\\n",
    "    --input_dataset \"./tmp/wikipedia_test_long\" \\\n",
    "    --output_dataset \"your_output_dir\" \\\n",
    "    --train_size 0 \\\n",
    "    --valid_size 0 \\\n",
    "    --test_size 5000 \\"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
