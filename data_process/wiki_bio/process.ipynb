{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# process origin dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load origin dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_bio = datasets.load_dataset(\"wiki_bio\")\n",
    "\n",
    "wiki_bio[\"valid\"] = wiki_bio[\"val\"]\n",
    "wiki_bio.pop(\"val\")\n",
    "\n",
    "wiki_bio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create union dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "union_dataset = datasets.combine.concatenate_datasets([wiki_bio[\"train\"], wiki_bio[\"test\"], wiki_bio[\"valid\"]])\n",
    "\n",
    "union_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check column name & select column for following experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text_list = union_dataset[\"input_text\"]\n",
    "\n",
    "type(input_text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_name_list = [\n",
    "    column_name \n",
    "    for input_text in input_text_list \n",
    "    for column_name in input_text[\"table\"][\"column_header\"]\n",
    "]\n",
    "\n",
    "len(column_name_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_name_counter = Counter(column_name_list)\n",
    "\n",
    "len(column_name_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_name_counter.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 2023\n",
    "attribute_list = [\"birth_date\", \"occupation\", \"nationality\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create new dataset with attribute_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def have_attributes(example):\n",
    "    for attribute in attribute_list:\n",
    "        if attribute not in example[\"input_text\"][\"table\"][\"column_header\"]:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def create_attributes(example):\n",
    "    result = {}\n",
    "    for attribute in attribute_list:\n",
    "        index = example[\"input_text\"][\"table\"][\"column_header\"].index(attribute)\n",
    "        value = example[\"input_text\"][\"table\"][\"content\"][index]\n",
    "        result[attribute] = value\n",
    "    return result\n",
    "\n",
    "\n",
    "dataset = datasets.DatasetDict(\n",
    "    {\n",
    "        \"train\": wiki_bio[\"train\"],\n",
    "        \"test\": wiki_bio[\"test\"],\n",
    "        \"valid\": wiki_bio[\"valid\"],\n",
    "    }\n",
    ")\n",
    "\n",
    "for name in dataset.keys():\n",
    "    dataset[name] = dataset[name].filter(have_attributes, num_proc=32)\n",
    "    dataset[name] = dataset[name].map(create_attributes, num_proc=32)\n",
    "    dataset[name] = dataset[name].remove_columns([\"input_text\"])\n",
    "    dataset[name] = dataset[name].rename_columns({\"target_text\": \"text\"})\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "union_dataset = datasets.combine.concatenate_datasets([dataset[\"train\"], dataset[\"test\"], dataset[\"valid\"]])\n",
    "\n",
    "union_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# process new dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preocess birth_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "birth_date_counter = Counter(union_dataset[\"birth_date\"])\n",
    "\n",
    "len(birth_date_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "birth_date_counter.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_year_from_string(input_string):\n",
    "    # Define the regular expression pattern to match the year\n",
    "    pattern = r'\\b\\d{4}\\b'\n",
    "\n",
    "    # Use regular expression to find all matches of the year in the input string\n",
    "    matches = re.findall(pattern, input_string)\n",
    "\n",
    "    # If there are multiple matches, return the first matched year\n",
    "    if matches:\n",
    "        return matches[0]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "for name in dataset.keys():\n",
    "    dataset[name] = dataset[name].map(lambda example: {\"birth_date\": extract_year_from_string(example[\"birth_date\"])}, num_proc=32)\n",
    "    dataset[name] = dataset[name].filter(lambda example: example[\"birth_date\"] is not None, num_proc=32)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "union_dataset = datasets.combine.concatenate_datasets([dataset[\"train\"], dataset[\"test\"], dataset[\"valid\"]])\n",
    "\n",
    "union_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## process occupation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "occupation_counter = Counter(union_dataset[\"occupation\"])\n",
    "\n",
    "len(occupation_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "occupation_counter.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_occupation(example):\n",
    "    if \"-lrb-\" in example[\"occupation\"] or \"-rrb-\" in example[\"occupation\"]:\n",
    "        return False\n",
    "    if any(char.isdigit() for char in example[\"occupation\"]):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "for name in dataset.keys():\n",
    "    dataset[name] = dataset[name].filter(filter_occupation, num_proc=32)\n",
    "    \n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_occupation_again(example):\n",
    "    occupation_list = re.split('; |, |and |/ |&', example[\"occupation\"])\n",
    "    occupation_list = [x.strip() for x in occupation_list]\n",
    "    text = example[\"text\"].split(\"\\n\")[0]\n",
    "    for occupation in occupation_list:\n",
    "        if occupation in text:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "for name in dataset.keys():\n",
    "    dataset[name] = dataset[name].filter(filter_occupation_again, num_proc=32)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_occupation(example):\n",
    "    occupation_list = re.split('; |, |and |/ |&', example[\"occupation\"])\n",
    "    occupation_list = [x.strip() for x in occupation_list]\n",
    "    text = example[\"text\"].split(\"\\n\")[0]\n",
    "    for occupation in occupation_list:\n",
    "        if occupation in text:\n",
    "            break\n",
    "    return {\"occupation\": occupation}\n",
    "\n",
    "for name in dataset.keys():\n",
    "    dataset = dataset.map(process_occupation, num_proc=32)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "union_dataset = datasets.combine.concatenate_datasets([dataset[\"train\"], dataset[\"test\"], dataset[\"valid\"]])\n",
    "\n",
    "union_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## process nationality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_nationaity(example):\n",
    "    text = example[\"text\"].split(\"\\n\")[0]\n",
    "    if example[\"nationality\"] not in text:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "for name in dataset.keys():\n",
    "    dataset = dataset.filter(filter_nationaity, num_proc=32)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "union_dataset = datasets.combine.concatenate_datasets([dataset[\"train\"], dataset[\"test\"], dataset[\"valid\"]])\n",
    "\n",
    "union_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nationality_counter = Counter(union_dataset[\"nationality\"])\n",
    "\n",
    "len(nationality_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nationality_list = [\n",
    "    k\n",
    "    for k, v in nationality_counter.most_common(20)\n",
    "]\n",
    "\n",
    "nationality_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_nationality_again(example):\n",
    "    if example[\"nationality\"] not in nationality_list:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "for name in dataset.keys():\n",
    "    dataset[name] = dataset[name].filter(filter_nationality_again, num_proc=32)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "union_dataset = datasets.combine.concatenate_datasets([dataset[\"train\"], dataset[\"test\"], dataset[\"valid\"]])\n",
    "\n",
    "union_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## process text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_text(example):\n",
    "    text = example[\"text\"]\n",
    "    text = text.split(\"\\n\")[0]\n",
    "    if example[\"birth_date\"] not in text:\n",
    "        return False\n",
    "    if example[\"occupation\"] not in text:\n",
    "        return False\n",
    "    if example[\"nationality\"] not in text:\n",
    "        return False\n",
    "    # if len(text.split()) < 20:\n",
    "    #     return False\n",
    "    return True\n",
    "\n",
    "for name in dataset.keys():\n",
    "    dataset[name] = dataset[name].filter(filter_text, num_proc=32)\n",
    "\n",
    "dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "spacy.prefer_gpu(0)\n",
    "nlp = spacy.load(\"en_core_web_trf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_list = [\"january\", \"february\", \"march\", \"april\", \"may\", \"june\", \"july\", \"august\", \"september\", \"october\", \"november\", \"december\"]\n",
    "\n",
    "def process_text(example):\n",
    "    text = example[\"text\"]\n",
    "    # * process bracket and punctuation\n",
    "    text = text.split(\"\\n\")[0].strip()\n",
    "    text = text.replace(\"-lrb- \", \"(\")\n",
    "    text = text.replace(\" -rrb-\", \")\")\n",
    "    text = text.replace(\" ,\", \",\")\n",
    "    text = text.replace(\" .\", \".\")\n",
    "    # * process nationality\n",
    "    for nationality in nationality_list:\n",
    "        if nationality in text:\n",
    "            text = text.replace(nationality, nationality.capitalize())\n",
    "    # * process month\n",
    "    for month in month_list:\n",
    "        if month in text:\n",
    "            text = text.replace(month, month.capitalize())\n",
    "    # * process person name\n",
    "    doc = nlp(text)\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"PERSON\":\n",
    "            text = text.replace(ent.text, ent.text.title())\n",
    "\n",
    "    return {\"text\": text}\n",
    "\n",
    "for name in dataset.keys():\n",
    "    dataset[name] = dataset[name].map(process_text)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "union_dataset = datasets.combine.concatenate_datasets([dataset[\"train\"], dataset[\"test\"], dataset[\"valid\"]])\n",
    "\n",
    "union_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create candidate list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(seed)\n",
    "\n",
    "candidate_num = 10\n",
    "\n",
    "def create_candidate(example):\n",
    "    target = example[attribute]\n",
    "    tmp_list = list(attribute_counter.keys())\n",
    "    tmp_list.remove(target)\n",
    "    candidate_list = [target] + random.sample(tmp_list, candidate_num - 1)\n",
    "\n",
    "    return {f\"{attribute}_candidate_list\": candidate_list}\n",
    "\n",
    "\n",
    "for attribute in attribute_list:\n",
    "    attribute_counter = Counter(union_dataset[attribute])\n",
    "    for name in dataset.keys():\n",
    "        dataset[name] = dataset[name].map(create_candidate, num_proc=32)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "union_dataset = datasets.combine.concatenate_datasets([dataset[\"train\"], dataset[\"valid\"], dataset[\"test\"]])\n",
    "\n",
    "union_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save tmp dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "union_dataset.save_to_disk(\"./tmp/wiki_bio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get embedding & save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sup-simcse-bert-base-uncased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!torchrun --nproc_per_node=8 ../embedding/sup-simcse-bert-base-uncased.py \\\n",
    "    --input_dataset \"./tmp/wiki_bio\" \\\n",
    "    --output_dataset \"your_output_dir\" \\\n",
    "    --train_size 11786 \\\n",
    "    --valid_size 1532 \\\n",
    "    --test_size 1480 \\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e5-large-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!torchrun --nproc_per_node=8 ../embedding/e5-large-v2.py \\\n",
    "    --input_dataset \"./tmp/wiki_bio\" \\\n",
    "    --output_dataset \"your_output_dir\" \\\n",
    "    --train_size 11786 \\\n",
    "    --valid_size 1532 \\\n",
    "    --test_size 1480 \\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bge-large-en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!torchrun --nproc_per_node=8 ../embedding/bge-large-en.py \\\n",
    "    --input_dataset \"./tmp/wiki_bio\" \\\n",
    "    --output_dataset \"your_output_dir\" \\\n",
    "    --train_size 11786 \\\n",
    "    --valid_size 1532 \\\n",
    "    --test_size 1480 \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
